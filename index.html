<!DOCTYPE HTML>
<!--
	Stellar by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Unintended Bias in Machine Learning</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header" class="alt">
						<span class="logo"><img src="images/logo.jpg" alt="" height=200 width=200/></span>
						<h1>Unintended Bias in Machine Learning</h1>
						<p>What it is, why you should care, and what you can do.</p>
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul>
							<li><a href="#intro" class="active">What is unintended bias?</a></li>
							<li><a href="#first">Sources of unintended bias</a></li>
							<li><a href="#second">What problems are we facing?</a></li>
							<li><a href="#cta">What You Can Do</a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">

						<!-- Introduction -->
							<section id="intro" class="main">
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>What is unintended bias?</h2>
										</header>
                                        <p>In supervised machine learning, we <i>train</i> (teach) the machine to make inferences
                                        about data we haven't yet shown to it. We do this by feeding the machine training data,
                                        example data that is already tagged with the correct answer. For example, I could
                                        use a basket of fruit to teach the machine that round, red fruits are
                                        apples and cylindrical, yellow fruits are bananas. The machine develops a <i>model</i>
                                        based on the relationships and patterns between attributes of the examples. Then, if I
                                        present a banana from a different basket to the machine, it will classify the fruit as
                                        a banana based on its color and shape as a banana.<br><br>
                                        When supervised machine learning is used to make predictions about someone's behavior,
                                        unintended bias can occur. Unintended bias is a special kind of failure where the model
                                        makes better predictions for members of some demographic groups than others.</p>
										<!--<ul class="actions">
											<li><a href="generic.html" class="button">Learn More</a></li>
										</ul>-->
									</div>
									<span class="image"><img src="images/pic01.jpg" alt="" /></span>
								</div>
							</section>

						<!-- First Section -->
							<section id="first" class="main special">
								<header class="major">
									<h2>Sources of unintended bias</h2>
									<p>Understanding sources of unintended bias can help system designers recognize and mitigate them.</p>
								</header>
								<!--<ul class="statistics">
									<li class="style1">
										<span class="icon fa-code-fork"></span>
										<strong>5,120</strong> Etiam
									</li>
									<li class="style2">
										<span class="icon fa-folder-open-o"></span>
										<strong>8,192</strong> Magna
									</li>
									<li class="style3">
										<span class="icon fa-signal"></span>
										<strong>2,048</strong> Tempus
									</li>
									<li class="style4">
										<span class="icon fa-laptop"></span>
										<strong>4,096</strong> Aliquam
									</li>
									<li class="style5">
										<span class="icon fa-diamond"></span>
										<strong>1,024</strong> Nullam
									</li>
								</ul>-->
								<p class="content"><b>Training data bias</b> appears when we teach a model using human-generated examples. If the examples contain human biases, this can skew the predictions that are made. For example, a <a href="https://arxiv.org/pdf/1607.06520.pdf">system designed to predict word associations</a> might be trained using English text from news articles. Without interventions to mitigate bias, the model would associate "homemaker" with "female" and "computer programmer" with "male".<br><br>
                                <b>Algorithmic focus bias</b> is introduced when a system designer deliberately chooses to use or exclude certain types of information in decision-making. [TODO: including] In some cases, the deliberate exclusion of certain data attributes may result in unfair decisions. For example, researchers trained a machine learning model using a data set of individuals' FICO credit scores, racial membership, and whether the individual defaulted on their loan. They found that when using a race-blind FICO credit score threshold to decide whether to grant loans, black non-defaulters were <a href="https://arxiv.org/pdf/1610.02413.pdf">much less likely to qualify for a loan</a> than Asian or white ones. (<a href="http://research.google.com/bigpicture/attacking-discrimination-in-ml/">interactive visualization</a>)<br><br>
                                <b>Transfer context bias</b> occurs when a machine learning algorithm is applied outside of the context it for which it was intended. Machine learning systems in healthcare are one instance where the models are likely to contain this bias: they may perform poorly in rural clinics if developed for research hospitals.<br><br>
                                <b>Interpretation bias</b> is a misjudgment of how to apply the results from a machine learning algorithm. This can happen when machine learning is used to solve a small problem as part of a larger goal. As an example, a tool designed to tell a judge the likelihood that an individual will commit repeated crimes <a href="http://law.emory.edu/elj/content/volume-67/issue-1/articles/constructing-recidivism-risk.html">can't tell the judge what cut-off points to use</a> to categorize someone as high- or low-risk.</p>
								<footer class="major">
									<ul class="actions special">
										<li><a href="https://www.cmu.edu/dietrich/philosophy/docs/london/IJCAI17-AlgorithmicBias-Distrib.pdf" target="_blank" class="button">Learn More</a></li>
									</ul>
								</footer>
							</section>

						<!-- Second Section -->
							<section id="second" class="main special">
								<header class="major">
									<h2>What's the problem?</h2>
                                    <p>We are already seeing the harms.</p>
								</header>
								<ul class="features">
									<li>
                                        <div class="fadein">
										<span class="icon major style1 fa-briefcase"></span>
										<h3>Hiring</h3>
										<p>Amazon developed an <a href="https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G">experimental recruiting engine</a> to identify high-performing candidates from their resumes. The computer models were trained on past resumes submitted to the company. The project was shut down after engineers found that the system preferred resumes that included masculine language.</p>
                                        </div>
									</li>
									<li>
                                        <span class="icon major style5 fa-money"></span>
										<h3>Finance</h3>
										<p>Machine learning algorithms have led mortgage lenders to charge <a href="http://newsroom.haas.berkeley.edu/minority-homebuyers-face-widespread-statistical-lending-discrimination-study-finds/">higher interest rates to black and Latino borrowers</a>, in spite of creditworthiness.</p>
									</li>
									<li>
										<span class="icon major style3 fa-shield"></span>
										<h3>Predictive Policing</h3>
										<p>Predictive policing systems in Chicago and New Orleans were informed by <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3333423">"dirty data"</a> -- police records from periods of biased police practices. Flawed predictions can lead police to target particular neighborhoods at increasingly disproportionate rates.</p>
									</li>
                                    <!--<li>
                                        <span class="icon major style3 fa-gavel"></span>
                                        <h3>Criminal Sentencing</h3>
                                        <p>Sample text</p>
                                    </li>-->
									<li class="big">
										<span class="icon major style2 fa-line-chart"></span>
										<h3>Future Risks</h3>
										<p>Researchers are hard at work developing systems that apply machine learning to new areas such as medicine and law. Experts warn that unless decision-makers at technology companies implement <a href="https://ainowinstitute.org/AI_Now_2018_Report.pdf">internal governance strategies</a>, machine learning will play an increasingly large role in disadvantaging marginalized groups.</p>
									</li>
								</ul>
								<!--<footer class="major">
									<ul class="actions special">
										<li><a href="generic.html" class="button">Learn More</a></li>
									</ul>
								</footer>-->
							</section>

						<!-- Get Started -->
							<section id="cta" class="main special">
								<header class="major">
									<h2>What You Can Do</h2>
									<p>It may be possible to mitigate these unintended biases when they are discovered.<br />
									There are three layers in a system where interventions can happen.</p>
                                    <div class="container"><div class="accordion">
                                        <div class="accordion-item">
                                            <h3 class="accordion-item__title">Data Layer</h3>
                                            <div class="accordion-item__content">
                                                <div class="accordion-item__content-inside">
                                                    In situations where training data bias exists, it may be appropriate to remove examples from the training data that you don't want to influence the predictions. In other situations, you might choose to add additional examples so that the algorithm bases its predictions off a more representative data set.
                                                </div>
                                            </div>
                                        </div>
                                        <div class="accordion-item">
                                            <h3 class="accordion-item__title">Model Layer</h3>
                                            <div class="accordion-item__content">
                                                <div class="accordion-item__content-inside">
                                                    Consider machine learning algorithms that produce more fair outcomes for groups that are underrepresented in your training data. For example,
                                                </div>
                                            </div>
                                        </div>
                                        <div class="accordion-item">
                                            <h3 class="accordion-item__title">Deployment Layer</h3>
                                            <div class="accordion-item__content">
                                                <div class="accordion-item__content-inside">
                                                    When a false prediction could negatively impact an individual, consider having a human double check the predictions made by your machine learning model before you act on them. For example, if you are writing a filter to delete abusive messages on a message board, you might use the predictions from the model to flag messages for further inspection. Making decisions directly based off of the algorithm's predictions without human oversight has a higher risk of harm.
                                                </div>
                                            </div>
                                        </div>
                                    </div></div>
								</header>
								<footer class="major">
									<ul class="actions special">
										<!--<li><a href="generic.html" class="button primary">Get Started</a></li>-->
										<li><a href="https://ai.google.com/education/responsible-ai-practices?category=fairness" class="button">Learn More</a></li>
									</ul>
								</footer>
							</section>

					</div>

				<!-- Footer -->
					<footer id="footer">
						<section>
							<h2>About this site</h2>
							<p>This project was created in Spring 2019 as part of <a href="https://timlibert.me">Professor Timothy Libert</a>'s Information Technology Policy class at Carnegie Mellon University.</p>
							<!--<ul class="actions">
								<li><a href="generic.html" class="button">Learn More</a></li>
							</ul>-->
						</section>
						<section>
							<h2>Created by Liam Kirsh</h2>
							<dl class="alt">
								<dt>Email</dt>
								<dd><a href="mailto:liam@cmu.edu">liam@cmu.edu</a></dd>
							</dl>
							<ul class="icons">
                                <li><a href="https://medium.com/@liamkirsh" class="icon fa-medium alt"><span class="label">Medium</span></a></li>
								<li><a href="https://twitter.com/choicefresh" class="icon fa-twitter alt"><span class="label">Twitter</span></a></li>
								<li><a href="https://github.com/choicefresh" class="icon fa-github alt"><span class="label">GitHub</span></a></li>
							</ul>
						</section>
						<p class="copyright">&copy; Liam Kirsh. This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.<br>Design: <a href="https://html5up.net">HTML5 UP</a>.</p>
					</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>
            <script src="assets/js/index.js"></script>

	</body>
</html>
