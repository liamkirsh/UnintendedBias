<!DOCTYPE HTML>
<!--
	Stellar by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Unintended Bias in Machine Learning</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header" class="alt">
						<span class="logo"><img src="images/logo.jpg" alt="" height=200 width=200/></span>
						<h1>Unintended Bias in Machine Learning</h1>
						<p>What it is, why you should care, and what you can do.</p>
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul>
							<li><a href="#intro" class="active">What is unintended bias?</a></li>
							<li><a href="#first">What sources does unintended bias have?</a></li>
							<li><a href="#second">What problems are we facing?</a></li>
							<li><a href="#cta">What you can do</a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">

						<!-- Introduction -->
							<section id="intro" class="main">
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>What is unintended bias?</h2>
										</header>
                                        <p>In supervised machine learning, we <i>train</i> (teach) the machine to make inferences
                                        about data we haven't yet shown to it. We do this by feeding the machine training data,
                                        example data that is already tagged with the correct answer. For example, I could
                                        use a basket of fruit to teach the machine that round, red fruits are
                                        apples and cylindrical, yellow fruits are bananas. The machine develops a <i>model</i>
                                        based on the relationships and patterns between attributes of the examples. Then, if I
                                        present a banana from a different basket to the machine, it will classify the fruit as
                                        a banana based on its color and shape as a banana.<br><br>
                                        When supervised machine learning is used to make predictions about someone's behavior,
                                        unintended bias can occur. Unintended bias is a special kind of failure where the model
                                        makes better predictions for members of some demographic groups than others.</p>
										<!--<ul class="actions">
											<li><a href="generic.html" class="button">Learn More</a></li>
										</ul>-->
									</div>
									<span class="image"><img src="images/pic01.jpg" alt="" /></span>
								</div>
							</section>

						<!-- First Section -->
							<section id="first" class="main special">
								<header class="major">
									<h2>What sources does unintended bias have?</h2>
									<p>Understanding sources of unintended bias can help system designers recognize and mitigate them.</p>
								</header>
								<!--<ul class="statistics">
									<li class="style1">
										<span class="icon fa-code-fork"></span>
										<strong>5,120</strong> Etiam
									</li>
									<li class="style2">
										<span class="icon fa-folder-open-o"></span>
										<strong>8,192</strong> Magna
									</li>
									<li class="style3">
										<span class="icon fa-signal"></span>
										<strong>2,048</strong> Tempus
									</li>
									<li class="style4">
										<span class="icon fa-laptop"></span>
										<strong>4,096</strong> Aliquam
									</li>
									<li class="style5">
										<span class="icon fa-diamond"></span>
										<strong>1,024</strong> Nullam
									</li>
								</ul>-->
								<p class="content"><b>Training data bias</b> appears when we teach a model using human-generated examples. If the examples contain human biases, this can skew the predictions that are made. For example, a <a href="https://arxiv.org/pdf/1607.06520.pdf">system designed to predict word associations</a> might be trained using English text from news articles. Without interventions to mitigate bias, the model would associate "homemaker" with "female" and "computer programmer" with "male".<br><br>
                                <b>Algorithmic focus bias</b> is introduced when a system designer deliberately chooses not to use certain types of information in decision-making. In some cases, the deliberate non-use of certain data attributes may not ensure fairness in the way predictions are made. For example, researchers trained a machine learning model using a data set of individuals' FICO credit scores, racial membership, and whether the individual defaulted on their loan. They found that when using a race-blind FICO credit score threshold to decide whether to grant loans, black non-defaulters were <a href="https://arxiv.org/pdf/1610.02413.pdf">much less likely to qualify for a loan</a> than Asian or white ones. (<a href="http://research.google.com/bigpicture/attacking-discrimination-in-ml/">interactive visualization</a>)<br><br>
                                <b>Transfer context bias</b> occurs when a machine learning algorithm is applied outside of the context it for which it was intended. Integer maximus varius lorem, sed convallis diam accumsan sed.<br><br>
                                <b>Interpretation bias</b> Etiam porttitor placerat sapien, sed eleifend a enim pulvinar faucibus semper quis ut arcu. Ut non nisl a mollis est efficitur vestibulum. Integer eget purus nec nulla mattis et accumsan ut magna libero. Morbi auctor iaculis porttitor. Sed ut magna ac risus et hendrerit scelerisque. Praesent eleifend lacus in lectus aliquam porta. Cras eu ornare dui curabitur lacinia.</p>
								<footer class="major">
									<ul class="actions special">
										<li><a href="https://www.cmu.edu/dietrich/philosophy/docs/london/IJCAI17-AlgorithmicBias-Distrib.pdf" target="_blank" class="button">Learn More</a></li>
									</ul>
								</footer>
							</section>

						<!-- Second Section -->
							<section id="second" class="main special">
								<header class="major">
									<h2>What's the problem?</h2>
                                    <p>We are already seeing harms.</p>
								</header>
								<ul class="features">
									<li>
										<span class="icon major style1 fa-briefcase"></span>
										<h3>Hiring</h3>
										<p>Amazon developed an <a href="https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G">experimental recruiting engine</a> to identify high-performing candidates in resumes. The computer models were trained on past resumes submitted to the company. The project was abandoned after engineers found that the system preferred resumes that included masculine language.</p>
									</li>
									<li>
										<span class="icon major style5 fa-money"></span>
										<h3>Finance</h3>
										<p>Machine learning algorithms have led mortgage lenders to charge <a href="http://newsroom.haas.berkeley.edu/minority-homebuyers-face-widespread-statistical-lending-discrimination-study-finds/">higher interest rates to Black and Latino borrowers</a>, in spite of creditworthiness.</p>
									</li>
									<li>
										<span class="icon major style3 fa-shield"></span>
										<h3>Predictive Policing</h3>
										<p>Predictive policing systems in Chicago and New Orleans were informed by <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3333423">"dirty data"</a> -- police records from periods of biased police practices. Flawed predictions can lead police to target particular neighborhoods at increasingly disproportionate rates.</p>
									</li>
									<li class="big">
										<span class="icon major style2 fa-line-chart"></span>
										<h3>Future Risks</h3>
										<p>Researchers are hard at work developing systems that apply machine learning to make predictions in areas such as medicine and law. Experts warn that unless decision-makers at technology companies develop <a href="https://ainowinstitute.org/AI_Now_2018_Report.pdf">internal governance strategies</a>, machine learning will play an increasingly large role in disadvantaging marginalized groups.</p>
									</li>
								</ul>
								<!--<footer class="major">
									<ul class="actions special">
										<li><a href="generic.html" class="button">Learn More</a></li>
									</ul>
								</footer>-->
							</section>

						<!-- Get Started -->
							<section id="cta" class="main special">
								<header class="major">
									<h2>What You Can Do</h2>
									<p>Donec imperdiet consequat consequat. Suspendisse feugiat congue<br />
									posuere. Nulla massa urna, fermentum eget quam aliquet.</p>
								</header>
								<footer class="major">
									<ul class="actions special">
										<li><a href="generic.html" class="button primary">Get Started</a></li>
										<li><a href="generic.html" class="button">Learn More</a></li>
									</ul>
								</footer>
							</section>

					</div>

				<!-- Footer -->
					<footer id="footer">
						<section>
							<h2>About this site</h2>
							<p>This project was created in Spring 2019 as part of <a href="https://timlibert.me">Professor Timothy Libert</a>'s Information Technology Policy class at Carnegie Mellon University.</p>
							<!--<ul class="actions">
								<li><a href="generic.html" class="button">Learn More</a></li>
							</ul>-->
						</section>
						<section>
							<h2>Created by Liam Kirsh</h2>
							<dl class="alt">
								<dt>Email</dt>
								<dd><a href="mailto:liam@cmu.edu">liam@cmu.edu</a></dd>
							</dl>
							<ul class="icons">
                                <li><a href="https://medium.com/@liamkirsh" class="icon fa-medium alt"><span class="label">Medium</span></a></li>
								<li><a href="https://twitter.com/choicefresh" class="icon fa-twitter alt"><span class="label">Twitter</span></a></li>
								<li><a href="https://github.com/choicefresh" class="icon fa-github alt"><span class="label">GitHub</span></a></li>
							</ul>
						</section>
						<p class="copyright">&copy; Liam Kirsh. This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.<br>Design: <a href="https://html5up.net">HTML5 UP</a>.</p>
					</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
